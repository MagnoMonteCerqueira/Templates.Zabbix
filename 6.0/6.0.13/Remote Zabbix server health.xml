<?xml version="1.0" encoding="UTF-8"?>
<zabbix_export><version>6.0</version><date>2023-02-11T16:14:40Z</date><groups><group><uuid>a571c0d144b14fd4a87a9d9b2aa9fcd6</uuid><name>Templates/Applications</name></group></groups><templates><template><uuid>79b16cbbe593444eae3de66de0cb566b</uuid><template>Remote Zabbix server health</template><name>Remote Zabbix server health</name><description>Template tooling version used: 0.41</description><groups><group><name>Templates/Applications</name></group></groups><items><item><uuid>adcf44e5179644b0bee34103d3d3688e</uuid><name>Remote Zabbix server: LLD queue</name><type>DEPENDENT</type><key>lld_queue</key><delay>0</delay><history>1w</history><description>Count of values enqueued in the low-level discovery processing queue.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.lld_queue</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>9881f258f69d459195b5a19c56c523a8</uuid><name>Remote Zabbix server: Preprocessing queue</name><type>DEPENDENT</type><key>preprocessing_queue</key><delay>0</delay><history>1w</history><description>Count of values enqueued in the preprocessing queue.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.preprocessing_queue</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>d8071d9c280c468cb70b19be50c756a5</uuid><name>Remote Zabbix server: Utilization of alerter internal processes, in %</name><type>DEPENDENT</type><key>process.alerter.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time alerter processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['alerter'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes alerter not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>d4aae59fe9294da5b1c724b051af1683</uuid><expression>avg(/Remote Zabbix server health/process.alerter.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.alerter.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of alerter processes is high</name><event_name>Remote Zabbix server: Utilization of alerter processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>14347f8309934849a2ac13974e4b66d1</uuid><name>Remote Zabbix server: Utilization of alert manager internal processes, in %</name><type>DEPENDENT</type><key>process.alert_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time alert manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['alert manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes alert manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>310f33d488ae4f5faa245c96f8dc583c</uuid><expression>avg(/Remote Zabbix server health/process.alert_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.alert_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of alert manager processes is high</name><event_name>Remote Zabbix server: Utilization of alert manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>236699e6400c4ae3a20868c79fd35027</uuid><name>Remote Zabbix server: Utilization of alert syncer internal processes, in %</name><type>DEPENDENT</type><key>process.alert_syncer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time alert syncer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['alert syncer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes alert syncer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>0eb23de05fac4aeab4ee4aa337545b16</uuid><expression>avg(/Remote Zabbix server health/process.alert_syncer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.alert_syncer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of alert syncer processes is high</name><event_name>Remote Zabbix server: Utilization of alert syncer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>44ae7bbbca5346a4926bf1208e00deb8</uuid><name>Remote Zabbix server: Utilization of availability manager internal processes, in %</name><type>DEPENDENT</type><key>process.availability_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time availability manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['availability manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes availability manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>ae06acfebdb3452b88359f1634c53876</uuid><expression>avg(/Remote Zabbix server health/process.availability_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.availability_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of availability manager processes is high</name><event_name>Remote Zabbix server: Utilization of availability manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>439e81aed41e4fd7ba7f2e4a97ca1a91</uuid><name>Remote Zabbix server: Utilization of configuration syncer internal processes, in %</name><type>DEPENDENT</type><key>process.configuration_syncer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time configuration syncer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['configuration syncer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes configuration syncer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>f1caa410beea429eb91daaec8f027eb3</uuid><expression>avg(/Remote Zabbix server health/process.configuration_syncer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.configuration_syncer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of configuration syncer processes is high</name><event_name>Remote Zabbix server: Utilization of configuration syncer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>396cdd296a3a4ee4a247afaa0f64e41c</uuid><name>Remote Zabbix server: Utilization of discoverer data collector processes, in %</name><type>DEPENDENT</type><key>process.discoverer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time discoverer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['discoverer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes discoverer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>2ed92415c6c542639a8b8d1f0a4646f4</uuid><expression>avg(/Remote Zabbix server health/process.discoverer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.discoverer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of discoverer processes is high</name><event_name>Remote Zabbix server: Utilization of discoverer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>d8110a23217142b9b10f2eda00a96f39</uuid><name>Remote Zabbix server: Utilization of escalator internal processes, in %</name><type>DEPENDENT</type><key>process.escalator.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time escalator processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['escalator'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes escalator not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>de07be1b10d9446abed4a387dd4980dd</uuid><expression>avg(/Remote Zabbix server health/process.escalator.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.escalator.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of escalator processes is high</name><event_name>Remote Zabbix server: Utilization of escalator processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>edefe2732df14bc3933b8e1ff65fbd21</uuid><name>Remote Zabbix server: Utilization of history poller data collector processes, in %</name><type>DEPENDENT</type><key>process.history_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time history poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['history poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes history poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>655c59a6151d4ce985a833d9f91efc2a</uuid><expression>avg(/Remote Zabbix server health/process.history_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.history_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of history poller processes is high</name><event_name>Remote Zabbix server: Utilization of history poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>2b651866c6f04880888a770e9a08a167</uuid><name>Remote Zabbix server: Utilization of history syncer internal processes, in %</name><type>DEPENDENT</type><key>process.history_syncer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time history syncer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['history syncer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes history syncer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>b8e6a6ce3667459bad05afa850e99bc0</uuid><expression>avg(/Remote Zabbix server health/process.history_syncer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.history_syncer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of history syncer processes is high</name><event_name>Remote Zabbix server: Utilization of history syncer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>8eb5b40740a349fbb14ed39dd840fc85</uuid><name>Remote Zabbix server: Utilization of housekeeper internal processes, in %</name><type>DEPENDENT</type><key>process.housekeeper.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time housekeeper processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['housekeeper'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes housekeeper not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>b879623a755e4b79972c2ad3189886a2</uuid><expression>avg(/Remote Zabbix server health/process.housekeeper.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.housekeeper.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of housekeeper processes is high</name><event_name>Remote Zabbix server: Utilization of housekeeper processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>02371dc53214403dbf34ff9e970408ef</uuid><name>Remote Zabbix server: Utilization of http poller data collector processes, in %</name><type>DEPENDENT</type><key>process.http_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time http poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['http poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes http poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>f7b4217cdd4b46b1949a5e6ac5fc3d02</uuid><expression>avg(/Remote Zabbix server health/process.http_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.http_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of http poller processes is high</name><event_name>Remote Zabbix server: Utilization of http poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>3a2dfedfac164b69a01105137eefd5e9</uuid><name>Remote Zabbix server: Utilization of icmp pinger data collector processes, in %</name><type>DEPENDENT</type><key>process.icmp_pinger.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time icmp pinger processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['icmp pinger'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes icmp pinger not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>312d404ff20740f38385ebe7c3e36d8f</uuid><expression>avg(/Remote Zabbix server health/process.icmp_pinger.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.icmp_pinger.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of icmp pinger processes is high</name><event_name>Remote Zabbix server: Utilization of icmp pinger processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>52a05106bc5e4e05b04142827f394662</uuid><name>Remote Zabbix server: Utilization of ipmi manager internal processes, in %</name><type>DEPENDENT</type><key>process.ipmi_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time ipmi manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['ipmi manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes ipmi manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>3fe728ab8de9447c899b2a919b885cec</uuid><expression>avg(/Remote Zabbix server health/process.ipmi_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.ipmi_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of ipmi manager processes is high</name><event_name>Remote Zabbix server: Utilization of ipmi manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>849f8b11cc984c758c4947fe256e3310</uuid><name>Remote Zabbix server: Utilization of ipmi poller data collector processes, in %</name><type>DEPENDENT</type><key>process.ipmi_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time ipmi poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['ipmi poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes ipmi poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>dbf27d6a629a4a2cb75e04849b93ac0f</uuid><expression>avg(/Remote Zabbix server health/process.ipmi_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.ipmi_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of ipmi poller processes is high</name><event_name>Remote Zabbix server: Utilization of ipmi poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>e55654715f8143c597c858d7eb522e31</uuid><name>Remote Zabbix server: Utilization of java poller data collector processes, in %</name><type>DEPENDENT</type><key>process.java_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time java poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['java poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes java poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>304a2dcf77c8494fb18f52cc6bae73a9</uuid><expression>avg(/Remote Zabbix server health/process.java_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.java_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of java poller processes is high</name><event_name>Remote Zabbix server: Utilization of java poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>b8504e03317649399dabfffc2758d77c</uuid><name>Remote Zabbix server: Utilization of LLD manager internal processes, in %</name><type>DEPENDENT</type><key>process.lld_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time lld manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['lld manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes LLD manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>4027f813da2549cfb99e1bb5e04e1d28</uuid><expression>avg(/Remote Zabbix server health/process.lld_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.lld_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of lld manager processes is high</name><event_name>Remote Zabbix server: Utilization of lld manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>c4d708a59d314de6814f4936ec6b0a99</uuid><name>Remote Zabbix server: Utilization of LLD worker internal processes, in %</name><type>DEPENDENT</type><key>process.lld_worker.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time lld worker processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['lld worker'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes LLD worker not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>ef1e836e906e4cf79b9fc8a11818c0a6</uuid><expression>avg(/Remote Zabbix server health/process.lld_worker.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.lld_worker.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of lld worker processes is high</name><event_name>Remote Zabbix server: Utilization of lld worker processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>8a21a0281ae24e0788edc8c9b875a421</uuid><name>Remote Zabbix server: Utilization of ODBC poller data collector processes, in %</name><type>DEPENDENT</type><key>process.odbc_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time ODBC poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['odbc poller'].busy.avg</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>eadd70080005430c8b11666d7e93a095</uuid><expression>avg(/Remote Zabbix server health/process.odbc_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.odbc_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of ODBC poller processes is high</name><event_name>Remote Zabbix server: Utilization of ODBC poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>61df12fa054b46f4b50596807dbc90c3</uuid><name>Remote Zabbix server: Utilization of poller data collector processes, in %</name><type>DEPENDENT</type><key>process.poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>689ed5dd9bbd4873a825dfa675378d50</uuid><expression>avg(/Remote Zabbix server health/process.poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of poller processes is high</name><event_name>Remote Zabbix server: Utilization of poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>42e9efb8e5e54b26b84372dc53e823b3</uuid><name>Remote Zabbix server: Utilization of preprocessing manager internal processes, in %</name><type>DEPENDENT</type><key>process.preprocessing_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time preprocessing manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['preprocessing manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes preprocessing manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>b43f04a35e17415bb81bfb59357d45ac</uuid><expression>avg(/Remote Zabbix server health/process.preprocessing_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.preprocessing_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of preprocessing manager processes is high</name><event_name>Remote Zabbix server: Utilization of preprocessing manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>90bd4b46d63845b98a2c430aac157ea5</uuid><name>Remote Zabbix server: Utilization of preprocessing worker internal processes, in %</name><type>DEPENDENT</type><key>process.preprocessing_worker.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time preprocessing worker processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['preprocessing worker'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes preprocessing worker not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>1c28310eb72047449437ee94854029fa</uuid><expression>avg(/Remote Zabbix server health/process.preprocessing_worker.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.preprocessing_worker.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of preprocessing worker processes is high</name><event_name>Remote Zabbix server: Utilization of preprocessing worker processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>4c5eb5acc8a447afa47d732988dafbd9</uuid><name>Remote Zabbix server: Utilization of proxy poller data collector processes, in %</name><type>DEPENDENT</type><key>process.proxy_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time proxy poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['proxy poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes proxy poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>c0d2c7c008cb45ee8b032b7c83018ae1</uuid><expression>avg(/Remote Zabbix server health/process.proxy_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.proxy_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of proxy poller processes is high</name><event_name>Remote Zabbix server: Utilization of proxy poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>032c86725d7d479e8e0a8cf0ea14b349</uuid><name>Remote Zabbix server: Utilization of report manager internal processes, in %</name><type>DEPENDENT</type><key>process.report_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time report manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['report manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes report manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>32d6f18edd5a45e1999f294f227c600f</uuid><expression>avg(/Remote Zabbix server health/process.report_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.report_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of report manager processes is high</name><event_name>Remote Zabbix server: Utilization of report manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>0a91377a6d5a460aa716164b201db6c0</uuid><name>Remote Zabbix server: Utilization of report writer internal processes, in %</name><type>DEPENDENT</type><key>process.report_writer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time report writer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['report writer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes report writer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>83fa871d54f74c89a7c85d429d40da53</uuid><expression>avg(/Remote Zabbix server health/process.report_writer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.report_writer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of report writer processes is high</name><event_name>Remote Zabbix server: Utilization of report writer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>044bb995bcae42568979fd958383fc60</uuid><name>Remote Zabbix server: Utilization of self-monitoring internal processes, in %</name><type>DEPENDENT</type><key>process.self-monitoring.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time self-monitoring processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['self-monitoring'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes self-monitoring not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>0d9b8d1f2d7d454ab472320ac172252c</uuid><expression>avg(/Remote Zabbix server health/process.self-monitoring.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.self-monitoring.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of self-monitoring processes is high</name><event_name>Remote Zabbix server: Utilization of self-monitoring processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>99da1d8bd47447d282542d40c1edea00</uuid><name>Remote Zabbix server: Utilization of service manager internal processes, in %</name><type>DEPENDENT</type><key>process.service_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time service manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['service manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes service manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>001c9e2027ca4e7c8f5dc5b78681c573</uuid><expression>avg(/Remote Zabbix server health/process.service_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.service_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of service manager processes is high</name><event_name>Remote Zabbix server: Utilization of service manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>643036d74a10414f94dc357efbbd42b2</uuid><name>Remote Zabbix server: Utilization of snmp trapper data collector processes, in %</name><type>DEPENDENT</type><key>process.snmp_trapper.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time snmp trapper processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['snmp trapper'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes snmp trapper not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>4726331c52284261aa3f347448c1c3c5</uuid><expression>avg(/Remote Zabbix server health/process.snmp_trapper.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.snmp_trapper.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of snmp trapper processes is high</name><event_name>Remote Zabbix server: Utilization of snmp trapper processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>7fad0515db154228b5baabb51d647a9f</uuid><name>Remote Zabbix server: Utilization of task manager internal processes, in %</name><type>DEPENDENT</type><key>process.task_manager.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time task manager processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['task manager'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes task manager not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>07d29e82b74a4cc5b0d701458af302b7</uuid><expression>avg(/Remote Zabbix server health/process.task_manager.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.task_manager.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of task manager processes is high</name><event_name>Remote Zabbix server: Utilization of task manager processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>105eae2d56d34d91b540f93009a575aa</uuid><name>Remote Zabbix server: Utilization of timer internal processes, in %</name><type>DEPENDENT</type><key>process.timer.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time timer processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['timer'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes timer not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>72f3b3f143a94f5a9b4b2c509425954e</uuid><expression>avg(/Remote Zabbix server health/process.timer.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.timer.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of timer processes is high</name><event_name>Remote Zabbix server: Utilization of timer processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>9c90b3a087e1495caa17a00bebb70145</uuid><name>Remote Zabbix server: Utilization of trapper data collector processes, in %</name><type>DEPENDENT</type><key>process.trapper.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time trapper processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['trapper'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes trapper not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>d31285f177224c4cb570373f6a730019</uuid><expression>avg(/Remote Zabbix server health/process.trapper.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.trapper.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of trapper processes is high</name><event_name>Remote Zabbix server: Utilization of trapper processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>56dca0db33264d4b9c8fd769fbdff7b5</uuid><name>Remote Zabbix server: Utilization of trigger housekeeper internal processes, in %</name><type>DEPENDENT</type><key>process.trigger_housekeeper.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time trigger housekeeper processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['trigger housekeeper'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes trigger housekeeper not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>internal-process</value></tag></tags><triggers><trigger><uuid>9c0daf6e85884122aa8c5b8432709c92</uuid><expression>avg(/Remote Zabbix server health/process.trigger_housekeeper.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.trigger_housekeeper.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of trigger housekeeper processes is high</name><event_name>Remote Zabbix server: Utilization of trigger housekeeper processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>8d43954f6b9843009353c560a4f84157</uuid><name>Remote Zabbix server: Utilization of unreachable poller data collector processes, in %</name><type>DEPENDENT</type><key>process.unreachable_poller.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time unreachable poller processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['unreachable poller'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes unreachable poller not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>d254ccddc7dd4e1ebf1eaa5113270e85</uuid><expression>avg(/Remote Zabbix server health/process.unreachable_poller.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.unreachable_poller.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of unreachable poller processes is high</name><event_name>Remote Zabbix server: Utilization of unreachable poller processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>9f02a181362b42b882d869353fae93ce</uuid><name>Remote Zabbix server: Utilization of vmware data collector processes, in %</name><type>DEPENDENT</type><key>process.vmware_collector.avg.busy</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Average percentage of time vmware collector processes have been busy in the last minute</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.process['vmware collector'].busy.avg</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Processes vmware collector not started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>data-collector</value></tag></tags><triggers><trigger><uuid>5929cfbe23494a41a20a4c05854f9578</uuid><expression>avg(/Remote Zabbix server health/process.vmware_collector.avg.busy,10m)&gt;75</expression><recovery_mode>RECOVERY_EXPRESSION</recovery_mode><recovery_expression>avg(/Remote Zabbix server health/process.vmware_collector.avg.busy,10m)&lt;65</recovery_expression><name>Remote Zabbix server: Utilization of vmware collector processes is high</name><event_name>Remote Zabbix server: Utilization of vmware collector processes over 75%</event_name><priority>AVERAGE</priority><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>750964513e1d44d7b138980d6cac2188</uuid><name>Remote Zabbix server: Configuration cache, % used</name><type>DEPENDENT</type><key>rcache.buffer.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Availability statistics of Zabbix configuration cache. Percentage of used buffer.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.rcache.pused</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>9d5f36b486ab4781a5f58210143785e9</uuid><expression>max(/Remote Zabbix server health/rcache.buffer.pused,10m)&gt;75</expression><name>Remote Zabbix server: More than 75% used in the configuration cache</name><priority>AVERAGE</priority><description>Consider increasing CacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>1625b3470fbf4bd7aa5b7051e328b37c</uuid><name>Remote Zabbix server: Trend function cache, % unique requests</name><type>DEPENDENT</type><key>tcache.pitems</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Effectiveness statistics of the Zabbix trend function cache. Percentage
of cached items from cached items + requests. Low percentage most likely means
that the cache size can be reduced.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.tcache.pitems</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Not supported this version</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>2c8bf95e97dd48679e34591c72626c90</uuid><name>Remote Zabbix server: Trend function cache, % misses</name><type>DEPENDENT</type><key>tcache.pmisses</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Effectiveness statistics of the Zabbix trend function cache. Percentage of cache misses.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.tcache.pmisses</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>Not supported this version</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>9f9d8d1fcd08498698e37b5889502b47</uuid><name>Remote Zabbix server: Value cache, % used</name><type>DEPENDENT</type><key>vcache.buffer.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Availability statistics of Zabbix value cache. Percentage of used buffer.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.vcache.buffer.pused</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>d35671cd02034aeba3c7c71ed618f4ee</uuid><expression>max(/Remote Zabbix server health/vcache.buffer.pused,10m)&gt;95</expression><name>Remote Zabbix server: More than 95% used in the value cache</name><priority>AVERAGE</priority><description>Consider increasing ValueCacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>9bd0079126974bf2a61f552de2cdf880</uuid><name>Remote Zabbix server: Value cache hits</name><type>DEPENDENT</type><key>vcache.cache.hits</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>vps</units><description>Effectiveness statistics of Zabbix value cache. Number of cache hits
(history values taken from the cache).</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.vcache.cache.hits</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>e8c372778d4e434e98d3fba5ea603a50</uuid><name>Remote Zabbix server: Value cache misses</name><type>DEPENDENT</type><key>vcache.cache.misses</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>vps</units><description>Effectiveness statistics of Zabbix value cache. Number of cache misses
(history values taken from the database).</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.vcache.cache.misses</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>1a1daeb7267f4ed4bf75c70699d487b6</uuid><name>Remote Zabbix server: Value cache operating mode</name><type>DEPENDENT</type><key>vcache.cache.mode</key><delay>0</delay><history>1w</history><description>Value cache operating mode.</description><valuemap><name>Value cache operating mode</name></valuemap><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.vcache.cache.mode</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>95e8823159fc4baeb13d656be176ac16</uuid><expression>last(/Remote Zabbix server health/vcache.cache.mode)=1</expression><name>Remote Zabbix server: Zabbix value cache working in low memory mode</name><priority>HIGH</priority><description>Once the low memory mode has been switched on, the value cache will remain in this state for 24 hours, even if the problem that triggered this mode is resolved sooner.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>f0eab81aaf464b809ea4d487548e57c6</uuid><name>Remote Zabbix server: Version</name><type>DEPENDENT</type><key>version</key><delay>0</delay><history>1w</history><trends>0</trends><value_type>CHAR</value_type><description>Version of Zabbix server.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.version</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>1d</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>93a52f22c79b40539fcd3a26183daad9</uuid><expression>last(/Remote Zabbix server health/version,#1)&lt;&gt;last(/Remote Zabbix server health/version,#2) and length(last(/Remote Zabbix server health/version))&gt;0</expression><name>Remote Zabbix server: Version has changed</name><event_name>Remote Zabbix server: Version has changed (new version: {ITEM.VALUE})</event_name><priority>INFO</priority><description>Remote Zabbix server version has changed. Ack to close.</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>9e2b2133916e4ce29ed4e9ca07c74123</uuid><name>Remote Zabbix server: VMware cache, % used</name><type>DEPENDENT</type><key>vmware.buffer.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Availability statistics of Zabbix vmware cache. Percentage of used buffer.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.vmware.pused</parameter></parameters><error_handler>CUSTOM_ERROR</error_handler><error_handler_params>No vmware collector processes started</error_handler_params></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>08e6c62dd52b4a0694be910c4f4fa997</uuid><expression>max(/Remote Zabbix server health/vmware.buffer.pused,10m)&gt;75</expression><name>Remote Zabbix server: More than 75% used in the vmware cache</name><priority>AVERAGE</priority><description>Consider increasing VMwareCacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>98f47a07e2ba4294a9eb2aa1df2ba9f3</uuid><name>Remote Zabbix server: History write cache, % used</name><type>DEPENDENT</type><key>wcache.history.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Statistics and availability of Zabbix write cache. Percentage of used history buffer.
History cache is used to store item values. A high number indicates performance problems on the database side.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.history.pused</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>a66914bd7f2e4442a8b555deacebb4c9</uuid><expression>max(/Remote Zabbix server health/wcache.history.pused,10m)&gt;75</expression><name>Remote Zabbix server: More than 75% used in the history cache</name><priority>AVERAGE</priority><description>Consider increasing HistoryCacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>b1add70210a44b668f06cc3e062173ad</uuid><name>Remote Zabbix server: History index cache, % used</name><type>DEPENDENT</type><key>wcache.index.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Statistics and availability of Zabbix write cache. Percentage of used history index buffer.
History index cache is used to index values stored in history cache.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.index.pused</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>6aedbc1768ed4f949890248af98fdaa2</uuid><expression>max(/Remote Zabbix server health/wcache.index.pused,10m)&gt;75</expression><name>Remote Zabbix server: More than 75% used in the history index cache</name><priority>AVERAGE</priority><description>Consider increasing HistoryIndexCacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>1f79208bb2ec4e4eb26e6e1b95c3635e</uuid><name>Remote Zabbix server: Trend write cache, % used</name><type>DEPENDENT</type><key>wcache.trend.pused</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><units>%</units><description>Statistics and availability of Zabbix write cache. Percentage of used trend buffer.
Trend cache stores aggregate for the current hour for all items that receive data.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.trend.pused</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>742a4938fdd24026921b375a67cea921</uuid><expression>max(/Remote Zabbix server health/wcache.trend.pused,10m)&gt;75</expression><name>Remote Zabbix server: More than 75% used in the trends cache</name><priority>AVERAGE</priority><description>Consider increasing TrendCacheSize in the zabbix_server.conf configuration file.</description><tags><tag><tag>scope</tag><value>capacity</value></tag><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>34fe014843974248a98a45596ce43e1a</uuid><name>Remote Zabbix server: Number of processed values per second</name><type>DEPENDENT</type><key>wcache.values</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Total number of values processed by Zabbix server or Zabbix proxy, except unsupported items.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.all</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>206f4fcb1e6d4e468a172e330f56b8aa</uuid><name>Remote Zabbix server: Number of processed numeric (float) values per second</name><type>DEPENDENT</type><key>wcache.values.float</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of processed float values.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.float</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>e2f3dc320d54446b80862eabe6fa0be5</uuid><name>Remote Zabbix server: Number of processed log values per second</name><type>DEPENDENT</type><key>wcache.values.log</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of processed log values.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.log</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>82e088caf6e14c1cab700ad7fcc96263</uuid><name>Remote Zabbix server: Number of processed not supported values per second</name><type>DEPENDENT</type><key>wcache.values.not_supported</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of times item processing resulted in item becoming unsupported or keeping that state.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values['not supported']</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>99415cd1920a4e7f8b99f3c71e7eba97</uuid><name>Remote Zabbix server: Number of processed character values per second</name><type>DEPENDENT</type><key>wcache.values.str</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of processed character/string values.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.str</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>7172378f31a44a36a095a2c2afed763b</uuid><name>Remote Zabbix server: Number of processed text values per second</name><type>DEPENDENT</type><key>wcache.values.text</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of processed text values.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.text</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>7865ba7a928945f4a148acf623b72537</uuid><name>Remote Zabbix server: Number of processed numeric (unsigned) values per second</name><type>DEPENDENT</type><key>wcache.values.uint</key><delay>0</delay><history>1w</history><value_type>FLOAT</value_type><description>Statistics and availability of Zabbix write cache.
Number of processed numeric (unsigned) values.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.wcache.values.uint</parameter></parameters></step><step><type>CHANGE_PER_SECOND</type><parameters><parameter/></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>95c8bab7640a4227b8ce6cac06e1a08b</uuid><name>Remote Zabbix server: Zabbix stats queue over 10m</name><type>INTERNAL</type><key>zabbix[stats,{$ADDRESS},{$PORT},queue,10m]</key><history>1w</history><description>Number of monitored items in the queue which are delayed at least by 10 minutes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.queue</parameter></parameters></step></preprocessing><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>c8eddec1bb9740bb8ca05f74bd023fcc</uuid><expression>min(/Remote Zabbix server health/zabbix[stats,{$ADDRESS},{$PORT},queue,10m],10m)&gt;100</expression><name>Remote Zabbix server: More than 100 items having missing data for more than 10 minutes</name><priority>WARNING</priority><description>zabbix[stats,{$IP},{$PORT},queue,10m] item is collecting data about
how many items are missing data for more than 10 minutes.</description><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>c140ebe6c0404ee6b17b5ada2de09f28</uuid><name>Remote Zabbix server: Zabbix stats queue</name><type>INTERNAL</type><key>zabbix[stats,{$ADDRESS},{$PORT},queue]</key><history>1w</history><description>Number of monitored items in the queue which are delayed at least by 6 seconds.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.queue</parameter></parameters></step></preprocessing><tags><tag><tag>component</tag><value>system</value></tag></tags></item><item><uuid>b9a588fee45047b6b991015e7e83f3f2</uuid><name>Remote Zabbix server: Zabbix stats</name><type>INTERNAL</type><key>zabbix[stats,{$ADDRESS},{$PORT}]</key><history>0d</history><trends>0</trends><value_type>TEXT</value_type><description>Zabbix server statistics master item.</description><tags><tag><tag>component</tag><value>raw</value></tag></tags></item></items><discovery_rules><discovery_rule><uuid>0557f879881a4d558410444737ae3738</uuid><name>High availability cluster node discovery</name><type>DEPENDENT</type><key>zabbix.nodes.discovery</key><delay>0</delay><description>LLD rule with item and trigger prototypes for node discovery.</description><item_prototypes><item_prototype><uuid>caca3ccdcec141a3a9c3233ba72b8d0d</uuid><name>Cluster node [{#NODE.NAME}]: Address</name><type>DEPENDENT</type><key>zabbix.nodes.address[{#NODE.ID}]</key><delay>0</delay><history>1w</history><trends>0</trends><value_type>CHAR</value_type><description>Node IPv4 address.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.ha[?(@.id==&quot;{#NODE.ID}&quot;)].address.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>12h</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>cluster</value></tag><tag><tag>node-id</tag><value>{#NODE.ID}</value></tag><tag><tag>node-name</tag><value>{#NODE.NAME}</value></tag></tags></item_prototype><item_prototype><uuid>a9ded61c4dba4abb9213aec9af00a604</uuid><name>Cluster node [{#NODE.NAME}]: Last access age</name><type>DEPENDENT</type><key>zabbix.nodes.lastaccess.age[{#NODE.ID}]</key><delay>0</delay><history>1w</history><units>uptime</units><description>Time between database unix_timestamp() and last access time.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.ha[?(@.id==&quot;{#NODE.ID}&quot;)].lastaccess_age.first()</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>cluster</value></tag><tag><tag>node-id</tag><value>{#NODE.ID}</value></tag><tag><tag>node-name</tag><value>{#NODE.NAME}</value></tag></tags></item_prototype><item_prototype><uuid>0968e4fe51eb4c2b98a508090014aba0</uuid><name>Cluster node [{#NODE.NAME}]: Last access time</name><type>DEPENDENT</type><key>zabbix.nodes.lastaccess.time[{#NODE.ID}]</key><delay>0</delay><history>1w</history><units>unixtime</units><description>Last access time.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.ha[?(@.id==&quot;{#NODE.ID}&quot;)].lastaccess.first()</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>cluster</value></tag><tag><tag>node-id</tag><value>{#NODE.ID}</value></tag><tag><tag>node-name</tag><value>{#NODE.NAME}</value></tag></tags></item_prototype><item_prototype><uuid>2b629362886546cab34396352b93835a</uuid><name>Cluster node [{#NODE.NAME}]: Status</name><type>DEPENDENT</type><key>zabbix.nodes.status[{#NODE.ID}]</key><delay>0</delay><history>1w</history><description>Node status.</description><valuemap><name>Cluster node status</name></valuemap><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.ha[?(@.id==&quot;{#NODE.ID}&quot;)].status.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>12h</parameter></parameters></step></preprocessing><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><tags><tag><tag>component</tag><value>cluster</value></tag><tag><tag>node-id</tag><value>{#NODE.ID}</value></tag><tag><tag>node-name</tag><value>{#NODE.NAME}</value></tag></tags><trigger_prototypes><trigger_prototype><uuid>5d68b55175924cb4adde600a019496c4</uuid><expression>last(/Remote Zabbix server health/zabbix.nodes.status[{#NODE.ID}],#1)&lt;&gt;last(/Remote Zabbix server health/zabbix.nodes.status[{#NODE.ID}],#2)</expression><name>Cluster node [{#NODE.NAME}]: Status changed</name><opdata>Current value: {ITEM.LASTVALUE1}</opdata><priority>INFO</priority><description>The state of the node has changed. Confirm to close.</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype></item_prototypes><master_item><key>zabbix[stats,{$ADDRESS},{$PORT}]</key></master_item><lld_macro_paths><lld_macro_path><lld_macro>{#NODE.ID}</lld_macro><path>$.id</path></lld_macro_path><lld_macro_path><lld_macro>{#NODE.NAME}</lld_macro><path>$.name</path></lld_macro_path></lld_macro_paths><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.data.ha</parameter></parameters></step></preprocessing></discovery_rule></discovery_rules><tags><tag><tag>class</tag><value>software</value></tag><tag><tag>target</tag><value>server</value></tag><tag><tag>target</tag><value>zabbix</value></tag></tags><macros><macro><macro>{$ADDRESS}</macro></macro><macro><macro>{$PORT}</macro></macro></macros><dashboards><dashboard><uuid>acf492c7fb70451ba2afc6938e745a3a</uuid><name>Zabbix server health</name><pages><page><widgets><widget><type>GRAPH_CLASSIC</type><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Zabbix server performance</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><x>12</x><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Zabbix data gathering process busy %</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><y>5</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Zabbix internal process busy %</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><x>12</x><y>5</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Zabbix cache usage, % free</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><y>10</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Value cache effectiveness</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><x>12</x><y>10</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Zabbix internal queues</name></value></field></fields></widget></widgets></page></pages></dashboard><dashboard><uuid>6b2f06c17f804d97b3ff5a2fb4cae3a0</uuid><name>Zabbix server processes</name><pages><page><widgets><widget><type>GRAPH_CLASSIC</type><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Data handling processes</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><x>12</x><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Reporting processes</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><y>5</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Internal processes</name></value></field></fields></widget><widget><type>GRAPH_CLASSIC</type><x>12</x><y>5</y><width>12</width><height>5</height><fields><field><type>INTEGER</type><name>source_type</name><value>0</value></field><field><type>GRAPH</type><name>graphid</name><value><host>Remote Zabbix server health</host><name>Remote Zabbix server: Data gathering processes</name></value></field></fields></widget></widgets></page></pages></dashboard></dashboards><valuemaps><valuemap><uuid>25ab0f5c570b4a7e9d15bd41db79fe25</uuid><name>Cluster node status</name><mappings><mapping><value>0</value><newvalue>Standby</newvalue></mapping><mapping><value>1</value><newvalue>Stopped</newvalue></mapping><mapping><value>2</value><newvalue>Unavailable</newvalue></mapping><mapping><value>3</value><newvalue>Active</newvalue></mapping></mappings></valuemap><valuemap><uuid>5dff563dde3c45d8b6d92525111384c6</uuid><name>Value cache operating mode</name><mappings><mapping><value>0</value><newvalue>Normal</newvalue></mapping><mapping><value>1</value><newvalue>Low memory</newvalue></mapping></mappings></valuemap></valuemaps></template></templates><graphs><graph><uuid>11e19e2e56fb4fb1ab0094a7c2906ed6</uuid><name>Remote Zabbix server: Data gathering processes</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>990099</color><item><host>Remote Zabbix server health</host><key>process.trapper.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>990000</color><item><host>Remote Zabbix server health</host><key>process.poller.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>0000EE</color><item><host>Remote Zabbix server health</host><key>process.ipmi_poller.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>FF33FF</color><item><host>Remote Zabbix server health</host><key>process.ipmi_manager.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><item><host>Remote Zabbix server health</host><key>process.icmp_pinger.avg.busy</key></item></graph_item><graph_item><sortorder>5</sortorder><color>003300</color><item><host>Remote Zabbix server health</host><key>process.http_poller.avg.busy</key></item></graph_item><graph_item><sortorder>6</sortorder><color>33FFFF</color><item><host>Remote Zabbix server health</host><key>process.unreachable_poller.avg.busy</key></item></graph_item><graph_item><sortorder>7</sortorder><color>DD0000</color><item><host>Remote Zabbix server health</host><key>process.java_poller.avg.busy</key></item></graph_item><graph_item><sortorder>8</sortorder><color>000099</color><item><host>Remote Zabbix server health</host><key>process.snmp_trapper.avg.busy</key></item></graph_item><graph_item><sortorder>9</sortorder><color>00FF00</color><item><host>Remote Zabbix server health</host><key>process.vmware_collector.avg.busy</key></item></graph_item><graph_item><sortorder>10</sortorder><color>5A2B57</color><item><host>Remote Zabbix server health</host><key>process.history_poller.avg.busy</key></item></graph_item><graph_item><sortorder>11</sortorder><color>9FA8DA</color><item><host>Remote Zabbix server health</host><key>process.odbc_poller.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>e15b35d8270e4085812b9b74369fb048</uuid><name>Remote Zabbix server: Data handling processes</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>00EE00</color><item><host>Remote Zabbix server health</host><key>process.history_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>0000EE</color><item><host>Remote Zabbix server health</host><key>process.preprocessing_manager.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>FFAA00</color><item><host>Remote Zabbix server health</host><key>process.preprocessing_worker.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>00EEEE</color><item><host>Remote Zabbix server health</host><key>process.lld_worker.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><color>990099</color><item><host>Remote Zabbix server health</host><key>process.lld_manager.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>d35b696819e3402e9a9f8bd98da01bde</uuid><name>Remote Zabbix server: Internal processes</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>00EE00</color><item><host>Remote Zabbix server health</host><key>process.timer.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>FFAA00</color><item><host>Remote Zabbix server health</host><key>process.housekeeper.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>990099</color><item><host>Remote Zabbix server health</host><key>process.configuration_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>FF66FF</color><item><host>Remote Zabbix server health</host><key>process.self-monitoring.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><color>009999</color><item><host>Remote Zabbix server health</host><key>process.task_manager.avg.busy</key></item></graph_item><graph_item><sortorder>5</sortorder><color>2B5429</color><item><host>Remote Zabbix server health</host><key>process.availability_manager.avg.busy</key></item></graph_item><graph_item><sortorder>6</sortorder><color>8048B4</color><item><host>Remote Zabbix server health</host><key>process.discoverer.avg.busy</key></item></graph_item><graph_item><sortorder>7</sortorder><color>FD5434</color><item><host>Remote Zabbix server health</host><key>process.proxy_poller.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>c0a947a13faa422b9f4b96602b93a1c2</uuid><name>Remote Zabbix server: Reporting processes</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>00EE00</color><item><host>Remote Zabbix server health</host><key>process.escalator.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>0000EE</color><item><host>Remote Zabbix server health</host><key>process.alerter.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>FFAA00</color><item><host>Remote Zabbix server health</host><key>process.alert_manager.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>00EEEE</color><item><host>Remote Zabbix server health</host><key>process.alert_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><color>990099</color><item><host>Remote Zabbix server health</host><key>process.report_manager.avg.busy</key></item></graph_item><graph_item><sortorder>5</sortorder><color>EE0000</color><item><host>Remote Zabbix server health</host><key>process.report_writer.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>f90ef248dc4a4f0c9527fd95af1c6a9e</uuid><name>Remote Zabbix server: Trend function cache effectiveness</name><graph_items><graph_item><color>C80000</color><item><host>Remote Zabbix server health</host><key>tcache.pmisses</key></item></graph_item><graph_item><sortorder>1</sortorder><color>00C800</color><item><host>Remote Zabbix server health</host><key>tcache.pitems</key></item></graph_item></graph_items></graph><graph><uuid>0b1efe2dd8044ed19822bc15f34a6e67</uuid><name>Remote Zabbix server: Value cache effectiveness</name><type>STACKED</type><graph_items><graph_item><color>C80000</color><item><host>Remote Zabbix server health</host><key>vcache.cache.misses</key></item></graph_item><graph_item><sortorder>1</sortorder><color>00C800</color><item><host>Remote Zabbix server health</host><key>vcache.cache.hits</key></item></graph_item></graph_items></graph><graph><uuid>3b9bfc5fdc904721b154a011b15adde1</uuid><name>Remote Zabbix server: Zabbix cache usage, % free</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>009900</color><item><host>Remote Zabbix server health</host><key>wcache.trend.pused</key></item></graph_item><graph_item><sortorder>1</sortorder><color>DD0000</color><item><host>Remote Zabbix server health</host><key>rcache.buffer.pused</key></item></graph_item><graph_item><sortorder>2</sortorder><color>00DDDD</color><item><host>Remote Zabbix server health</host><key>wcache.index.pused</key></item></graph_item><graph_item><sortorder>3</sortorder><color>3333FF</color><item><host>Remote Zabbix server health</host><key>wcache.history.pused</key></item></graph_item><graph_item><sortorder>4</sortorder><color>999900</color><item><host>Remote Zabbix server health</host><key>vcache.buffer.pused</key></item></graph_item><graph_item><sortorder>5</sortorder><color>00FF00</color><item><host>Remote Zabbix server health</host><key>vmware.buffer.pused</key></item></graph_item></graph_items></graph><graph><uuid>a0cd0a1f4dc14209ae3e140fc2dad135</uuid><name>Remote Zabbix server: Zabbix data gathering process busy %</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>990099</color><item><host>Remote Zabbix server health</host><key>process.trapper.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>990000</color><item><host>Remote Zabbix server health</host><key>process.poller.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>0000EE</color><item><host>Remote Zabbix server health</host><key>process.ipmi_poller.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>FF33FF</color><item><host>Remote Zabbix server health</host><key>process.discoverer.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><item><host>Remote Zabbix server health</host><key>process.icmp_pinger.avg.busy</key></item></graph_item><graph_item><sortorder>5</sortorder><color>003300</color><item><host>Remote Zabbix server health</host><key>process.http_poller.avg.busy</key></item></graph_item><graph_item><sortorder>6</sortorder><color>CCCC00</color><item><host>Remote Zabbix server health</host><key>process.proxy_poller.avg.busy</key></item></graph_item><graph_item><sortorder>7</sortorder><color>33FFFF</color><item><host>Remote Zabbix server health</host><key>process.unreachable_poller.avg.busy</key></item></graph_item><graph_item><sortorder>8</sortorder><color>DD0000</color><item><host>Remote Zabbix server health</host><key>process.java_poller.avg.busy</key></item></graph_item><graph_item><sortorder>9</sortorder><color>000099</color><item><host>Remote Zabbix server health</host><key>process.snmp_trapper.avg.busy</key></item></graph_item><graph_item><sortorder>10</sortorder><color>00FF00</color><item><host>Remote Zabbix server health</host><key>process.vmware_collector.avg.busy</key></item></graph_item><graph_item><sortorder>11</sortorder><color>5A2B57</color><item><host>Remote Zabbix server health</host><key>process.history_poller.avg.busy</key></item></graph_item><graph_item><sortorder>12</sortorder><color>9FA8DA</color><item><host>Remote Zabbix server health</host><key>process.odbc_poller.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>dcf8ef4ebaf54a599aca040a0f6cd722</uuid><name>Remote Zabbix server: Zabbix internal process busy %</name><ymin_type_1>FIXED</ymin_type_1><ymax_type_1>FIXED</ymax_type_1><graph_items><graph_item><color>00EE00</color><item><host>Remote Zabbix server health</host><key>process.timer.avg.busy</key></item></graph_item><graph_item><sortorder>1</sortorder><color>0000EE</color><item><host>Remote Zabbix server health</host><key>process.escalator.avg.busy</key></item></graph_item><graph_item><sortorder>2</sortorder><color>FFAA00</color><item><host>Remote Zabbix server health</host><key>process.housekeeper.avg.busy</key></item></graph_item><graph_item><sortorder>3</sortorder><color>00EEEE</color><item><host>Remote Zabbix server health</host><key>process.alerter.avg.busy</key></item></graph_item><graph_item><sortorder>4</sortorder><color>990099</color><item><host>Remote Zabbix server health</host><key>process.configuration_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>5</sortorder><color>EE0000</color><item><host>Remote Zabbix server health</host><key>process.history_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>6</sortorder><color>FF66FF</color><item><host>Remote Zabbix server health</host><key>process.self-monitoring.avg.busy</key></item></graph_item><graph_item><sortorder>7</sortorder><color>009999</color><item><host>Remote Zabbix server health</host><key>process.task_manager.avg.busy</key></item></graph_item><graph_item><sortorder>8</sortorder><color>BBBB00</color><item><host>Remote Zabbix server health</host><key>process.ipmi_manager.avg.busy</key></item></graph_item><graph_item><sortorder>9</sortorder><color>AA0000</color><item><host>Remote Zabbix server health</host><key>process.alert_manager.avg.busy</key></item></graph_item><graph_item><sortorder>10</sortorder><color>990000</color><item><host>Remote Zabbix server health</host><key>process.preprocessing_manager.avg.busy</key></item></graph_item><graph_item><sortorder>11</sortorder><color>008800</color><item><host>Remote Zabbix server health</host><key>process.preprocessing_worker.avg.busy</key></item></graph_item><graph_item><sortorder>12</sortorder><color>80B0E0</color><item><host>Remote Zabbix server health</host><key>process.lld_manager.avg.busy</key></item></graph_item><graph_item><sortorder>13</sortorder><color>4080B0</color><item><host>Remote Zabbix server health</host><key>process.lld_worker.avg.busy</key></item></graph_item><graph_item><sortorder>14</sortorder><color>8000FF</color><item><host>Remote Zabbix server health</host><key>process.alert_syncer.avg.busy</key></item></graph_item><graph_item><sortorder>15</sortorder><color>2B5429</color><item><host>Remote Zabbix server health</host><key>process.availability_manager.avg.busy</key></item></graph_item><graph_item><sortorder>16</sortorder><color>8048B4</color><item><host>Remote Zabbix server health</host><key>process.report_manager.avg.busy</key></item></graph_item><graph_item><sortorder>17</sortorder><color>FD5434</color><item><host>Remote Zabbix server health</host><key>process.report_writer.avg.busy</key></item></graph_item><graph_item><sortorder>18</sortorder><color>790E1F</color><item><host>Remote Zabbix server health</host><key>process.service_manager.avg.busy</key></item></graph_item><graph_item><sortorder>19</sortorder><color>87AC4D</color><item><host>Remote Zabbix server health</host><key>process.trigger_housekeeper.avg.busy</key></item></graph_item></graph_items></graph><graph><uuid>63004d5061324a90b247209977de9cbd</uuid><name>Remote Zabbix server: Zabbix internal queues</name><graph_items><graph_item><color>008800</color><item><host>Remote Zabbix server health</host><key>preprocessing_queue</key></item></graph_item><graph_item><sortorder>1</sortorder><color>EE0000</color><item><host>Remote Zabbix server health</host><key>lld_queue</key></item></graph_item></graph_items></graph><graph><uuid>954f8afba30d45b99e3404114f0fe3b0</uuid><name>Remote Zabbix server: Zabbix server performance</name><graph_items><graph_item><drawtype>GRADIENT_LINE</drawtype><color>00C800</color><item><host>Remote Zabbix server health</host><key>wcache.values</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>Remote Zabbix server health</host><key>zabbix[stats,{$ADDRESS},{$PORT},queue]</key></item></graph_item></graph_items></graph></graphs></zabbix_export>
