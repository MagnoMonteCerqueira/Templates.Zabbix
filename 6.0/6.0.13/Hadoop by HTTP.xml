<?xml version="1.0" encoding="UTF-8"?>
<zabbix_export><version>6.0</version><date>2023-02-11T16:14:49Z</date><groups><group><uuid>a571c0d144b14fd4a87a9d9b2aa9fcd6</uuid><name>Templates/Applications</name></group></groups><templates><template><uuid>e129aeba7c814bf189772cf5919b4bbb</uuid><template>Hadoop by HTTP</template><name>Hadoop by HTTP</name><description>The template gets the Hadoop metrics from cluster's hosts (ResourceManager, NodeManagers, NameNode, DataNodes) by HTTP agent. You should define the IP address (or FQDN) and Web-UI port for the ResourceManager in {$HADOOP.RESOURCEMANAGER.HOST} and {$HADOOP.RESOURCEMANAGER.PORT} macros and for the NameNode in {$HADOOP.NAMENODE.HOST} and {$HADOOP.NAMENODE.PORT} macros respectively. Macros can be set in the template or overridden at the host level.

You can discuss this template or leave feedback on our forum https://www.zabbix.com/forum/zabbix-suggestions-and-feedback/413459-discussion-thread-for-official-zabbix-template-hadoop

Template tooling version used: 0.41</description><groups><group><name>Templates/Applications</name></group></groups><items><item><uuid>d2d19ac9d1eb434c98a55cbf76c27850</uuid><name>Get DataNodes states</name><type>HTTP_AGENT</type><key>hadoop.datanodes.get</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><preprocessing><step><type>JAVASCRIPT</type><parameters><parameter>try {
  parsed = JSON.parse(value);
  var result = [];

  function getNodes(nodes, state) {
      Object.keys(nodes).forEach(function (field) {
          var Node = {};
          Node['HostName'] = field || '';
          Node['adminState'] = nodes[field].adminState || '';
          Node['operState'] = state || '';
          Node['version'] = nodes[field].version || '';
          result.push(Node);
      });
  }

  getNodes(JSON.parse(parsed.beans[0].LiveNodes), 'Live');
  getNodes(JSON.parse(parsed.beans[0].DeadNodes), 'Dead');
  getNodes(JSON.parse(parsed.beans[0].DecomNodes), 'Decommission');
  getNodes(JSON.parse(parsed.beans[0].EnteringMaintenanceNodes), 'Maintenance');

  return JSON.stringify(result);
}
catch (error) {
  throw 'Failed to process response received from Hadoop';
}</parameter></parameters></step></preprocessing><url>{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item><item><uuid>2cb55b7ed9cd41878dc985497f45e084</uuid><name>NameNode: Total blocks</name><type>DEPENDENT</type><key>hadoop.namenode.blocks_total</key><delay>0</delay><history>7d</history><description>Count of blocks tracked by NameNode.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].BlocksTotal.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>1d098dc6fa134053b6c6be0e7618092e</uuid><name>NameNode: Blocks allocable</name><type>DEPENDENT</type><key>hadoop.namenode.block_capacity</key><delay>0</delay><history>7d</history><description>Maximum number of blocks allocable.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].BlockCapacity.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>26ca0bbd18e04b49b9eb8d2a74f4fd15</uuid><name>NameNode: Capacity remaining</name><type>DEPENDENT</type><key>hadoop.namenode.capacity_remaining</key><delay>0</delay><history>7d</history><units>B</units><description>Available capacity.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].CapacityRemaining.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>c73c2b6c24b846e49bdb68c3f5a01419</uuid><name>NameNode: Corrupt blocks</name><type>DEPENDENT</type><key>hadoop.namenode.corrupt_blocks</key><delay>0</delay><history>7d</history><description>Number of corrupt blocks.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].CorruptBlocks.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>82198b21427a4e39a173369db42d9de3</uuid><name>NameNode: Total files</name><type>DEPENDENT</type><key>hadoop.namenode.files_total</key><delay>0</delay><history>7d</history><description>Total count of files tracked by the NameNode.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].FilesTotal.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>687406d06ce94a8291b2e72bb2f8bec4</uuid><name>Get NameNode stats</name><type>HTTP_AGENT</type><key>hadoop.namenode.get</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><url>{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item><item><uuid>30ee7e09067e4f00a4f26ad6c00454b2</uuid><name>NameNode: Missing blocks</name><type>DEPENDENT</type><key>hadoop.namenode.missing_blocks</key><delay>0</delay><history>7d</history><description>Number of missing blocks.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].MissingBlocks.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags><triggers><trigger><uuid>3b92daaaddb74105a5e57c4b381e3060</uuid><expression>min(/Hadoop by HTTP/hadoop.namenode.missing_blocks,15m)&gt;0</expression><name>NameNode: Cluster has missing blocks</name><priority>AVERAGE</priority><description>A missing block is far worse than a corrupt block, because a missing block cannot be recovered by copying a replica.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>3473bad0a7c94c8b9fd35cd4398e6215</uuid><name>NameNode: Dead DataNodes</name><type>DEPENDENT</type><key>hadoop.namenode.num_dead_data_nodes</key><delay>0</delay><history>7d</history><description>Count of dead DataNodes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].NumDeadDataNodes.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags><triggers><trigger><uuid>b2d1a26791aa4b16865b4410c50c7ceb</uuid><expression>min(/Hadoop by HTTP/hadoop.namenode.num_dead_data_nodes,5m)&gt;0</expression><name>NameNode: Cluster has DataNodes in Dead state</name><priority>AVERAGE</priority><description>The death of a DataNode causes a flurry of network activity, as the NameNode initiates replication of blocks lost on the dead nodes.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>398a8c95db3248b684f222fe7b912fe3</uuid><name>NameNode: Alive DataNodes</name><type>DEPENDENT</type><key>hadoop.namenode.num_live_data_nodes</key><delay>0</delay><history>7d</history><description>Count of alive DataNodes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].NumLiveDataNodes.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>15bcb22fdc7f4e2c8f24560ef641d63d</uuid><name>NameNode: Stale DataNodes</name><type>DEPENDENT</type><key>hadoop.namenode.num_stale_data_nodes</key><delay>0</delay><history>7d</history><description>DataNodes that do not send a heartbeat within 30 seconds are marked as &quot;stale&quot;.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].StaleDataNodes.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>b72d54b849fc48fd8e7cdacd75943c23</uuid><name>NameNode: Block Pool Renaming</name><type>DEPENDENT</type><key>hadoop.namenode.percent_block_pool_used</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=NameNodeInfo')].PercentBlockPoolUsed.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>3cfbf084a31b479c91be356556d43c0d</uuid><name>NameNode: Percent capacity remaining</name><type>DEPENDENT</type><key>hadoop.namenode.percent_remaining</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>%</units><description>Available capacity in percent.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=NameNodeInfo')].PercentRemaining.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags><triggers><trigger><uuid>3104295848c5497085f397b8f3e06ef6</uuid><expression>max(/Hadoop by HTTP/hadoop.namenode.percent_remaining,15m)&lt;{$HADOOP.CAPACITY_REMAINING.MIN.WARN}</expression><name>NameNode: Cluster capacity remaining is low</name><event_name>NameNode: Cluster capacity remaining is low (below {$HADOOP.CAPACITY_REMAINING.MIN.WARN}% for 15m)</event_name><priority>WARNING</priority><description>A good practice is to ensure that disk use never exceeds 80 percent capacity.</description><tags><tag><tag>scope</tag><value>capacity</value></tag></tags></trigger></triggers></item><item><uuid>a9e6c1e2f9544c71844785b4baa9c017</uuid><name>NameNode: RPC queue &amp; processing time</name><type>DEPENDENT</type><key>hadoop.namenode.rpc_processing_time_avg</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><description>Average time spent on processing RPC requests.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=RpcActivityForPort9000')].RpcProcessingTimeAvgTime.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>9f00149ef0c2444ebbc9327b24acd7b9</uuid><name>NameNode: Total load</name><type>DEPENDENT</type><key>hadoop.namenode.total_load</key><delay>0</delay><history>7d</history><description>The current number of concurrent file accesses (read/write) across all DataNodes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].TotalLoad.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>6abfe537a36646a0b10fe2c72586d249</uuid><name>NameNode: Transactions since last checkpoint</name><type>DEPENDENT</type><key>hadoop.namenode.transactions_since_last_checkpoint</key><delay>0</delay><history>7d</history><description>Total number of transactions since last checkpoint.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].TransactionsSinceLastCheckpoint.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>249098bbeb7a43cdac59f1297ca95104</uuid><name>NameNode: Under-replicated blocks</name><type>DEPENDENT</type><key>hadoop.namenode.under_replicated_blocks</key><delay>0</delay><history>7d</history><description>The number of blocks with insufficient replication.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].UnderReplicatedBlocks.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags></item><item><uuid>7e8769eb77304b6f9c6e1d5bbd420fd0</uuid><name>NameNode: Uptime</name><type>DEPENDENT</type><key>hadoop.namenode.uptime</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Runtime')].Uptime.first()</parameter></parameters></step><step><type>MULTIPLIER</type><parameters><parameter>0.001</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>9fac0ae651ab40a08551945eb0a93b68</uuid><expression>nodata(/Hadoop by HTTP/hadoop.namenode.uptime,30m)=1</expression><name>NameNode: Failed to fetch NameNode API page</name><event_name>NameNode: Failed to fetch NameNode API page (or no data for 30m)</event_name><priority>WARNING</priority><description>Zabbix has not received data for items for the last 30 minutes.</description><manual_close>YES</manual_close><dependencies><dependency><name>NameNode: Service is unavailable</name><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;])=0</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger><trigger><uuid>84d866bc0dc3486d9c5dc9beefec8d31</uuid><expression>last(/Hadoop by HTTP/hadoop.namenode.uptime)&lt;10m</expression><name>NameNode: Service has been restarted</name><event_name>NameNode: Service has been restarted (uptime &lt; 10m)</event_name><priority>INFO</priority><description>Uptime is less than 10 minutes</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>396eb8f791d54254b08ddee553d3d944</uuid><name>NameNode: Failed volumes</name><type>DEPENDENT</type><key>hadoop.namenode.volume_failures_total</key><delay>0</delay><history>7d</history><description>Number of failed volumes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NameNode,name=FSNamesystem')].VolumeFailuresTotal.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.namenode.get</key></master_item><tags><tag><tag>component</tag><value>namenode</value></tag></tags><triggers><trigger><uuid>fcf791b6d0594dbb9ddfc3f93bc94825</uuid><expression>min(/Hadoop by HTTP/hadoop.namenode.volume_failures_total,15m)&gt;0</expression><name>NameNode: Cluster has volume failures</name><priority>AVERAGE</priority><description>HDFS now allows for disks to fail in place, without affecting DataNode operations, until a threshold value is reached. This is set on each DataNode via the dfs.datanode.failed.volumes.tolerated property; it defaults to 0, meaning that any volume failure will shut down the DataNode; on a production cluster where DataNodes typically have 6, 8, or 12 disks, setting this parameter to 1 or 2 is typically the best practice.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>6d7546c5d15d4e478b2e87e35d5306b0</uuid><name>Get NodeManagers states</name><type>HTTP_AGENT</type><key>hadoop.nodemanagers.get</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><preprocessing><step><type>JAVASCRIPT</type><parameters><parameter>return JSON.stringify(JSON.parse(JSON.parse(value).beans[0].LiveNodeManagers))</parameter></parameters></step></preprocessing><url>{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx?qry=Hadoop:service=ResourceManager,name=RMNMInfo</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item><item><uuid>e693cff98ec74cc198ec6b5e973f116c</uuid><name>Get ResourceManager stats</name><type>HTTP_AGENT</type><key>hadoop.resourcemanager.get</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><url>{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item><item><uuid>63d4fe7384044027b08b99698355fd8b</uuid><name>ResourceManager: Active NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_active_nm</key><delay>0</delay><history>7d</history><description>Number of Active NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumActiveNMs.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags><triggers><trigger><uuid>eb02a30f45394e4d84d9d7239002ed40</uuid><expression>max(/Hadoop by HTTP/hadoop.resourcemanager.num_active_nm,5m)=0</expression><name>ResourceManager: Cluster has no active NodeManagers</name><priority>HIGH</priority><description>Cluster is unable to execute any jobs without at least one NodeManager.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>3fccfdd8738544ca8969ade842430fc8</uuid><name>ResourceManager: Decommissioned NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_decommissioned_nm</key><delay>0</delay><history>7d</history><description>Number of Decommissioned NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumDecommissionedNMs.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>9aad193a9e074575878e44aa96ff4237</uuid><name>ResourceManager: Decommissioning NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_decommissioning_nm</key><delay>0</delay><history>7d</history><description>Number of Decommissioning NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumDecommissioningNMs.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>c4bbf5295b2a44619e2b641468071f9b</uuid><name>ResourceManager: Lost NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_lost_nm</key><delay>0</delay><history>7d</history><description>Number of Lost NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumLostNMs.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>b7791ce30e8f4aa7b5eea2ee7ca7eef9</uuid><name>ResourceManager: Rebooted NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_rebooted_nm</key><delay>0</delay><history>7d</history><description>Number of Rebooted NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumRebootedNMs.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>666152b3bf544a29b9e58a9f417c0ab8</uuid><name>ResourceManager: Shutdown NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_shutdown_nm</key><delay>0</delay><history>7d</history><description>Number of Shutdown NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumShutdownNMs.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>e6aa4b4b29414f2fb1f06bd536552c1c</uuid><name>ResourceManager: Unhealthy NMs</name><type>DEPENDENT</type><key>hadoop.resourcemanager.num_unhealthy_nm</key><delay>0</delay><history>7d</history><description>Number of Unhealthy NodeManagers.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=ClusterMetrics')].NumUnhealthyNMs.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags><triggers><trigger><uuid>0f35a0fa7a404559a3df225b906f0653</uuid><expression>min(/Hadoop by HTTP/hadoop.resourcemanager.num_unhealthy_nm,15m)&gt;0</expression><name>ResourceManager: Cluster has unhealthy NodeManagers</name><priority>AVERAGE</priority><description>YARN considers any node with disk utilization exceeding the value specified under the property yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage (in yarn-site.xml) to be unhealthy. Ample disk space is critical to ensure uninterrupted operation of a Hadoop cluster, and large numbers of unhealthyNodes (the number to alert on depends on the size of your cluster) should be quickly investigated and resolved.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>c4c3195326e34ebcb57e5039beffce7c</uuid><name>ResourceManager: RPC queue &amp; processing time</name><type>DEPENDENT</type><key>hadoop.resourcemanager.rpc_processing_time_avg</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><description>Average time spent on processing RPC requests.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=ResourceManager,name=RpcActivityForPort8031')].RpcProcessingTimeAvgTime.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>resourcemanager</value></tag></tags></item><item><uuid>4e74ca69a84d441e95e2c20afd25fada</uuid><name>ResourceManager: Uptime</name><type>DEPENDENT</type><key>hadoop.resourcemanager.uptime</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Runtime')].Uptime.first()</parameter></parameters></step><step><type>MULTIPLIER</type><parameters><parameter>0.001</parameter></parameters></step></preprocessing><master_item><key>hadoop.resourcemanager.get</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><triggers><trigger><uuid>7d4d026992344602a199966a8308a571</uuid><expression>nodata(/Hadoop by HTTP/hadoop.resourcemanager.uptime,30m)=1</expression><name>ResourceManager: Failed to fetch ResourceManager API page</name><event_name>ResourceManager: Failed to fetch ResourceManager API page (or no data for 30m)</event_name><priority>WARNING</priority><description>Zabbix has not received data for items for the last 30 minutes.</description><manual_close>YES</manual_close><dependencies><dependency><name>ResourceManager: Service is unavailable</name><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;])=0</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger><trigger><uuid>ade7cc30a4184ef89ed896bae56e0b18</uuid><expression>last(/Hadoop by HTTP/hadoop.resourcemanager.uptime)&lt;10m</expression><name>ResourceManager: Service has been restarted</name><event_name>ResourceManager: Service has been restarted (uptime &lt; 10m)</event_name><priority>INFO</priority><description>Uptime is less than 10 minutes</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger></triggers></item><item><uuid>66a87b21d32c436bb2d2eb23ec328f91</uuid><name>NameNode: Service response time</name><type>SIMPLE</type><key>net.tcp.service.perf[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;]</key><history>7d</history><value_type>FLOAT</value_type><units>s</units><description>Hadoop NameNode API performance.</description><tags><tag><tag>component</tag><value>network</value></tag></tags><triggers><trigger><uuid>4e4a6ab28fe5492d8fe4e291b8a586dc</uuid><expression>min(/Hadoop by HTTP/net.tcp.service.perf[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;],5m)&gt;{$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN}</expression><name>NameNode: Service response time is too high</name><event_name>NameNode: Service response time is too high (over {$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN} for 5m)</event_name><priority>WARNING</priority><manual_close>YES</manual_close><dependencies><dependency><name>NameNode: Service is unavailable</name><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;])=0</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>98b11f1156dc472fbce27ca053e01d4e</uuid><name>ResourceManager: Service response time</name><type>SIMPLE</type><key>net.tcp.service.perf[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;]</key><history>7d</history><value_type>FLOAT</value_type><units>s</units><description>Hadoop ResourceManager API performance.</description><tags><tag><tag>component</tag><value>network</value></tag></tags><triggers><trigger><uuid>e8e55f4c7e9e4823927a8c1345d3b941</uuid><expression>min(/Hadoop by HTTP/net.tcp.service.perf[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;],5m)&gt;{$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN}</expression><name>ResourceManager: Service response time is too high</name><event_name>ResourceManager: Service response time is too high (over {$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN} for 5m)</event_name><priority>WARNING</priority><manual_close>YES</manual_close><dependencies><dependency><name>ResourceManager: Service is unavailable</name><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;])=0</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>performance</value></tag></tags></trigger></triggers></item><item><uuid>2c52d856e07e4524abf3c2ae4b47c6b6</uuid><name>NameNode: Service status</name><type>SIMPLE</type><key>net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;]</key><history>7d</history><description>Hadoop NameNode API port availability.</description><valuemap><name>Service state</name></valuemap><preprocessing><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>10m</parameter></parameters></step></preprocessing><tags><tag><tag>component</tag><value>health</value></tag><tag><tag>component</tag><value>network</value></tag></tags><triggers><trigger><uuid>f7e16c4ec91e4c04b13b73ee817c71d7</uuid><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.NAMENODE.HOST}&quot;,&quot;{$HADOOP.NAMENODE.PORT}&quot;])=0</expression><name>NameNode: Service is unavailable</name><priority>AVERAGE</priority><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger></triggers></item><item><uuid>615b75c42ebe471da798a0613667d499</uuid><name>ResourceManager: Service status</name><type>SIMPLE</type><key>net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;]</key><history>7d</history><description>Hadoop ResourceManager API port availability.</description><valuemap><name>Service state</name></valuemap><preprocessing><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>10m</parameter></parameters></step></preprocessing><tags><tag><tag>component</tag><value>health</value></tag><tag><tag>component</tag><value>network</value></tag></tags><triggers><trigger><uuid>a9ac7ede0c004fe18ab9f1fee36ad2b2</uuid><expression>last(/Hadoop by HTTP/net.tcp.service[&quot;tcp&quot;,&quot;{$HADOOP.RESOURCEMANAGER.HOST}&quot;,&quot;{$HADOOP.RESOURCEMANAGER.PORT}&quot;])=0</expression><name>ResourceManager: Service is unavailable</name><priority>AVERAGE</priority><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger></triggers></item></items><discovery_rules><discovery_rule><uuid>0f05e90a6fc547d18f291ae2264db9d1</uuid><name>Data node discovery</name><type>HTTP_AGENT</type><key>hadoop.datanode.discovery</key><delay>1h</delay><item_prototypes><item_prototype><uuid>ef570f8b37c545bd880b7df20bd19f06</uuid><name>{#HOSTNAME}: Admin state</name><type>DEPENDENT</type><key>hadoop.datanode.admin_state[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><description>Administrative state.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.[?(@.HostName=='{#HOSTNAME}')].adminState.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanodes.get</key></master_item><tags><tag><tag>component</tag><value>datanode</value></tag></tags></item_prototype><item_prototype><uuid>14904ca75991456784d2082c14b7ec88</uuid><name>{#HOSTNAME}: Used</name><type>DEPENDENT</type><key>hadoop.datanode.dfs_used[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>B</units><description>Used disk space.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=DataNode,name=FSDatasetState')].DfsUsed.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>capacity</value></tag></tags></item_prototype><item_prototype><uuid>6d2d030b3ddb41a394faede737329bbb</uuid><name>Hadoop DataNode {#HOSTNAME}: Get stats</name><type>HTTP_AGENT</type><key>hadoop.datanode.get[{#HOSTNAME}]</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><url>{#INFOADDR}/jmx</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item_prototype><item_prototype><uuid>01bc20e53e314089a55b270961062c00</uuid><name>{#HOSTNAME}: JVM Garbage collection time</name><type>DEPENDENT</type><key>hadoop.datanode.jvm.gc_time[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>!ms</units><description>The JVM garbage collection time in milliseconds.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=DataNode,name=JvmMetrics')].GcTimeMillis.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>datanode</value></tag></tags></item_prototype><item_prototype><uuid>4cae9eef95f24810a6607de5348b7b54</uuid><name>{#HOSTNAME}: JVM Heap usage</name><type>DEPENDENT</type><key>hadoop.datanode.jvm.mem_heap_used[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>!MB</units><description>The JVM heap usage in MBytes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=DataNode,name=JvmMetrics')].MemHeapUsedM.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>capacity</value></tag></tags></item_prototype><item_prototype><uuid>dc30742dba2e4e5d99ca237615ffaef3</uuid><name>{#HOSTNAME}: JVM Threads</name><type>DEPENDENT</type><key>hadoop.datanode.jvm.threads[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><description>The number of JVM threads.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Threading')].ThreadCount.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>datanode</value></tag></tags></item_prototype><item_prototype><uuid>57c00b46aef94c018806cdae43adfab5</uuid><name>{#HOSTNAME}: Number of failed volumes</name><type>DEPENDENT</type><key>hadoop.datanode.numfailedvolumes[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><description>Number of failed storage volumes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=DataNode,name=FSDatasetState')].NumFailedVolumes.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>datanode</value></tag></tags></item_prototype><item_prototype><uuid>a6541492d4f7426b8016d1a8932b87ce</uuid><name>{#HOSTNAME}: Oper state</name><type>DEPENDENT</type><key>hadoop.datanode.oper_state[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><description>Operational state.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.[?(@.HostName=='{#HOSTNAME}')].operState.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanodes.get</key></master_item><tags><tag><tag>component</tag><value>datanode</value></tag></tags><trigger_prototypes><trigger_prototype><uuid>9f657289a04041e5bcaa1947f62f607d</uuid><expression>last(/Hadoop by HTTP/hadoop.datanode.oper_state[{#HOSTNAME}])&lt;&gt;&quot;Live&quot;</expression><name>{#HOSTNAME}: DataNode has state {ITEM.VALUE}.</name><priority>AVERAGE</priority><description>The state is different from normal.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><uuid>5a46ec3c89eb40d4ad57cec2080c66f8</uuid><name>{#HOSTNAME}: Remaining</name><type>DEPENDENT</type><key>hadoop.datanode.remaining[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>B</units><description>Remaining disk space.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=DataNode,name=FSDatasetState')].Remaining.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>capacity</value></tag></tags></item_prototype><item_prototype><uuid>2ac19ff8ee7f480f9974be56ab06eaaf</uuid><name>{#HOSTNAME}: Uptime</name><type>DEPENDENT</type><key>hadoop.datanode.uptime[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Runtime')].Uptime.first()</parameter></parameters></step><step><type>MULTIPLIER</type><parameters><parameter>0.001</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanode.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><trigger_prototypes><trigger_prototype><uuid>3eccb9daf76f4bde88b424cf6f2d21f6</uuid><expression>nodata(/Hadoop by HTTP/hadoop.datanode.uptime[{#HOSTNAME}],30m)=1</expression><name>{#HOSTNAME}: Failed to fetch DataNode API page</name><event_name>{#HOSTNAME}: Failed to fetch DataNode API page (or no data for 30m)</event_name><priority>WARNING</priority><description>Zabbix has not received data for items for the last 30 minutes.</description><manual_close>YES</manual_close><dependencies><dependency><name>{#HOSTNAME}: DataNode has state {ITEM.VALUE}.</name><expression>last(/Hadoop by HTTP/hadoop.datanode.oper_state[{#HOSTNAME}])&lt;&gt;&quot;Live&quot;</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger_prototype><trigger_prototype><uuid>e40298d300764251abcf93d5df3d9a67</uuid><expression>last(/Hadoop by HTTP/hadoop.datanode.uptime[{#HOSTNAME}])&lt;10m</expression><name>{#HOSTNAME}: Service has been restarted</name><event_name>{#HOSTNAME}: Service has been restarted (uptime &lt; 10m)</event_name><priority>INFO</priority><description>Uptime is less than 10 minutes</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><uuid>62b4ca9b1e8a43aa89fbeb78ac16c8cf</uuid><name>{#HOSTNAME}: Version</name><type>DEPENDENT</type><key>hadoop.datanode.version[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><description>DataNode software version.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.[?(@.HostName=='{#HOSTNAME}')].version.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.datanodes.get</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item_prototype></item_prototypes><graph_prototypes><graph_prototype><uuid>c497416bcce1416ebcede7fc491ccdba</uuid><name>{#HOSTNAME}: DataNode {#HOSTNAME} DFS size</name><type>STACKED</type><graph_items><graph_item><drawtype>FILLED_REGION</drawtype><color>1A7C11</color><item><host>Hadoop by HTTP</host><key>hadoop.datanode.dfs_used[{#HOSTNAME}]</key></item></graph_item><graph_item><sortorder>1</sortorder><drawtype>FILLED_REGION</drawtype><color>2774A4</color><item><host>Hadoop by HTTP</host><key>hadoop.datanode.remaining[{#HOSTNAME}]</key></item></graph_item></graph_items></graph_prototype></graph_prototypes><url>{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo</url><preprocessing><step><type>JAVASCRIPT</type><parameters><parameter>try{
  parsed = JSON.parse(value);
  var result = [];

  function getNodes(nodes) {
      Object.keys(nodes).forEach(function (field) {
          var Node = {};
          Node['{#HOSTNAME}'] = field || '';
          Node['{#INFOADDR}'] = nodes[field].infoAddr || '';
          result.push(Node);
      });
  }

  getNodes(JSON.parse(parsed.beans[0].LiveNodes));
  getNodes(JSON.parse(parsed.beans[0].DeadNodes));
  getNodes(JSON.parse(parsed.beans[0].DecomNodes));
  getNodes(JSON.parse(parsed.beans[0].EnteringMaintenanceNodes));

  return JSON.stringify(result);
}
catch (error) {
  throw 'Failed to process response received from Hadoop.';
}</parameter></parameters></step></preprocessing></discovery_rule><discovery_rule><uuid>de2d5f97843345668bc0b8c8336b9c14</uuid><name>Node manager discovery</name><type>HTTP_AGENT</type><key>hadoop.nodemanager.discovery</key><delay>1h</delay><item_prototypes><item_prototype><uuid>ffa4704e099a4f1a8b49add245938501</uuid><name>{#HOSTNAME}: Available memory</name><type>DEPENDENT</type><key>hadoop.nodemanager.availablememory[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>!MB</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$[?(@.HostName=='{#HOSTNAME}')].AvailableMemoryMB.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanagers.get</key></master_item><tags><tag><tag>component</tag><value>memory</value></tag></tags></item_prototype><item_prototype><uuid>e8d0ea2c96b643f899e370ab73c5c262</uuid><name>{#HOSTNAME}: Container launch avg duration</name><type>DEPENDENT</type><key>hadoop.nodemanager.container_launch_duration_avg[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NodeManager,name=NodeManagerMetrics')].ContainerLaunchDurationAvgTime.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>23c89dfb26a34b77bf34fcf543f719f2</uuid><name>Hadoop NodeManager {#HOSTNAME}: Get stats</name><type>HTTP_AGENT</type><key>hadoop.nodemanager.get[{#HOSTNAME}]</key><history>0h</history><trends>0</trends><value_type>TEXT</value_type><url>{#NODEHTTPADDRESS}/jmx</url><tags><tag><tag>component</tag><value>raw</value></tag></tags></item_prototype><item_prototype><uuid>82e289c999a246a6bd1feb85349d0348</uuid><name>{#HOSTNAME}: JVM Garbage collection time</name><type>DEPENDENT</type><key>hadoop.nodemanager.jvm.gc_time[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>!ms</units><description>The JVM garbage collection time in milliseconds.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NodeManager,name=JvmMetrics')].GcTimeMillis.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>4032f0a266c44b34896e8179bbed2419</uuid><name>{#HOSTNAME}: JVM Heap usage</name><type>DEPENDENT</type><key>hadoop.nodemanager.jvm.mem_heap_used[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>!MB</units><description>The JVM heap usage in MBytes.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NodeManager,name=JvmMetrics')].MemHeapUsedM.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>d7485913b2db4e31a8f02f63f8c18913</uuid><name>{#HOSTNAME}: JVM Threads</name><type>DEPENDENT</type><key>hadoop.nodemanager.jvm.threads[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><description>The number of JVM threads.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Threading')].ThreadCount.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>662cafd31e194db8808c75789bf712eb</uuid><name>{#HOSTNAME}: Number of containers</name><type>DEPENDENT</type><key>hadoop.nodemanager.numcontainers[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><preprocessing><step><type>JSONPATH</type><parameters><parameter>$[?(@.HostName=='{#HOSTNAME}')].NumContainers.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanagers.get</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>01a5bcdbfc1c4a84a471738998aed372</uuid><name>{#HOSTNAME}: RPC queue &amp; processing time</name><type>DEPENDENT</type><key>hadoop.nodemanager.rpc_processing_time_avg[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><description>Average time spent on processing RPC requests.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='Hadoop:service=NodeManager,name=RpcActivityForPort8040')].RpcProcessingTimeAvgTime.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags></item_prototype><item_prototype><uuid>bab9c705d31e42ce9af65b396e18504b</uuid><name>{#HOSTNAME}: State</name><type>DEPENDENT</type><key>hadoop.nodemanager.state[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><description>State of the node - valid values are: NEW, RUNNING, UNHEALTHY, DECOMMISSIONING, DECOMMISSIONED, LOST, REBOOTED, SHUTDOWN.</description><preprocessing><step><type>JSONPATH</type><parameters><parameter>$[?(@.HostName=='{#HOSTNAME}')].State.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanagers.get</key></master_item><tags><tag><tag>component</tag><value>nodemanager</value></tag></tags><trigger_prototypes><trigger_prototype><uuid>8752a292093347fcb16d3f06dd97c5c3</uuid><expression>last(/Hadoop by HTTP/hadoop.nodemanager.state[{#HOSTNAME}])&lt;&gt;&quot;RUNNING&quot;</expression><name>{#HOSTNAME}: NodeManager has state {ITEM.VALUE}.</name><priority>AVERAGE</priority><description>The state is different from normal.</description><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><uuid>f8f6799130d34848a7dfb65815939c48</uuid><name>{#HOSTNAME}: Uptime</name><type>DEPENDENT</type><key>hadoop.nodemanager.uptime[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><value_type>FLOAT</value_type><units>s</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$.beans[?(@.name=='java.lang:type=Runtime')].Uptime.first()</parameter></parameters></step><step><type>MULTIPLIER</type><parameters><parameter>0.001</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanager.get[{#HOSTNAME}]</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags><trigger_prototypes><trigger_prototype><uuid>6f8a6308d4334dd9bebe7af2fa3fb831</uuid><expression>nodata(/Hadoop by HTTP/hadoop.nodemanager.uptime[{#HOSTNAME}],30m)=1</expression><name>{#HOSTNAME}: Failed to fetch NodeManager API page</name><event_name>{#HOSTNAME}: Failed to fetch NodeManager API page (or no data for 30m)</event_name><priority>WARNING</priority><description>Zabbix has not received data for items for the last 30 minutes.</description><manual_close>YES</manual_close><dependencies><dependency><name>{#HOSTNAME}: NodeManager has state {ITEM.VALUE}.</name><expression>last(/Hadoop by HTTP/hadoop.nodemanager.state[{#HOSTNAME}])&lt;&gt;&quot;RUNNING&quot;</expression></dependency></dependencies><tags><tag><tag>scope</tag><value>availability</value></tag></tags></trigger_prototype><trigger_prototype><uuid>05f3cf8ed34f4a708df508f0e50e119d</uuid><expression>last(/Hadoop by HTTP/hadoop.nodemanager.uptime[{#HOSTNAME}])&lt;10m</expression><name>{#HOSTNAME}: Service has been restarted</name><event_name>{#HOSTNAME}: Service has been restarted (uptime &lt; 10m)</event_name><priority>INFO</priority><description>Uptime is less than 10 minutes</description><manual_close>YES</manual_close><tags><tag><tag>scope</tag><value>notice</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><uuid>d92b66e61a5244a995693ab8aedee96e</uuid><name>{#HOSTNAME}: Used memory</name><type>DEPENDENT</type><key>hadoop.nodemanager.usedmemory[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><units>!MB</units><preprocessing><step><type>JSONPATH</type><parameters><parameter>$[?(@.HostName=='{#HOSTNAME}')].UsedMemoryMB.first()</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanagers.get</key></master_item><tags><tag><tag>component</tag><value>memory</value></tag></tags></item_prototype><item_prototype><uuid>c4d46de2d6d341f5a2c1826236f94e5e</uuid><name>{#HOSTNAME}: Version</name><type>DEPENDENT</type><key>hadoop.nodemanager.version[{#HOSTNAME}]</key><delay>0</delay><history>7d</history><trends>0</trends><value_type>CHAR</value_type><preprocessing><step><type>JSONPATH</type><parameters><parameter>$[?(@.HostName=='{#HOSTNAME}')].NodeManagerVersion.first()</parameter></parameters></step><step><type>DISCARD_UNCHANGED_HEARTBEAT</type><parameters><parameter>6h</parameter></parameters></step></preprocessing><master_item><key>hadoop.nodemanagers.get</key></master_item><tags><tag><tag>component</tag><value>system</value></tag></tags></item_prototype></item_prototypes><url>{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx?qry=Hadoop:service=ResourceManager,name=RMNMInfo</url><preprocessing><step><type>JAVASCRIPT</type><parameters><parameter>try {
  parsed = JSON.parse(value);
  var result = [];

  function getNodes(nodes) {
      Object.keys(nodes).forEach(function (field) {
          var Node = {};
          Node['{#HOSTNAME}'] = nodes[field].HostName || '';
          Node['{#NODEHTTPADDRESS}'] = nodes[field].NodeHTTPAddress || '';
          result.push(Node);
      });
  }

  getNodes(JSON.parse(parsed.beans[0].LiveNodeManagers));

  return JSON.stringify(result);
}
catch (error) {
  throw 'Failed to process response received from Hadoop.';
}</parameter></parameters></step></preprocessing></discovery_rule></discovery_rules><tags><tag><tag>class</tag><value>application</value></tag><tag><tag>target</tag><value>hadoop</value></tag></tags><macros><macro><macro>{$HADOOP.CAPACITY_REMAINING.MIN.WARN}</macro><value>20</value><description>The Hadoop cluster capacity remaining percent for trigger expression.</description></macro><macro><macro>{$HADOOP.NAMENODE.HOST}</macro><value>NameNode</value><description>The Hadoop NameNode host IP address or FQDN.</description></macro><macro><macro>{$HADOOP.NAMENODE.PORT}</macro><value>9870</value><description>The Hadoop NameNode Web-UI port.</description></macro><macro><macro>{$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN}</macro><value>10s</value><description>The Hadoop NameNode API page maximum response time in seconds for trigger expression.</description></macro><macro><macro>{$HADOOP.RESOURCEMANAGER.HOST}</macro><value>ResourceManager</value><description>The Hadoop ResourceManager host IP address or FQDN.</description></macro><macro><macro>{$HADOOP.RESOURCEMANAGER.PORT}</macro><value>8088</value><description>The Hadoop ResourceManager Web-UI port.</description></macro><macro><macro>{$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN}</macro><value>10s</value><description>The Hadoop ResourceManager API page maximum response time in seconds for trigger expression.</description></macro></macros><valuemaps><valuemap><uuid>6c967c4df18d4c7ebb0fd4be17df292a</uuid><name>Service state</name><mappings><mapping><value>0</value><newvalue>Down</newvalue></mapping><mapping><value>1</value><newvalue>Up</newvalue></mapping></mappings></valuemap></valuemaps></template></templates></zabbix_export>
